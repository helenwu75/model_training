"""
Binary Model Training and Evaluation
-------------------------------------

This script trains and evaluates classification models for the binary prediction task of whether the market correctly predicted the election outcome. 
- outcome variable: prediction_correct a binary indicator of whether the market correctly predicted the election outcome (1 = correct prediction, 0 = incorrect prediction)

It uses the preprocessed data generated by data_cleaning.py and trains four models: Random Forest, Logistic Regression (L1 regularization), Gradient Boosting, and PCA + Logistic Regression. The script saves the following results:
- Model comparison table with accuracy, ROC AUC, and average precision
- ROC curves for all models
- Feature importance plots for Random Forest, Logistic Regression, and Gradient Boosting
- PCA explained variance plot and top PCA loadings
- SHAP summary plot for the best model
- Comparison of feature importance rankings across methods

The results are saved to a directory named model_results with a timestamped subdirectory.

Usage: python binary_model_training.py

Note: Make sure to run data_cleaning.py first to generate the preprocessed files
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import precision_recall_curve, average_precision_score, roc_auc_score, roc_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
import joblib
import shap
from datetime import datetime

# Create output directory for results
RESULTS_DIR = "model_results"
os.makedirs(RESULTS_DIR, exist_ok=True)
print(f"Results will be saved to: {RESULTS_DIR}")

# Create a timestamp for this run
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
print(f"Analysis started at: {timestamp}")

# Path to modified analysis folder
MODIFIED_ANALYSIS_DIR = "modified_analysis"

# 1. Load the preprocessed data
print("Loading preprocessed data...")
try:
    X_train = np.load(os.path.join(MODIFIED_ANALYSIS_DIR, 'X_train_preprocessed.npy'))
    X_test = np.load(os.path.join(MODIFIED_ANALYSIS_DIR, 'X_test_preprocessed.npy'))
    y_train_correct = np.load(os.path.join(MODIFIED_ANALYSIS_DIR, 'y_train_prediction_correct.npy'))
    y_test_correct = np.load(os.path.join(MODIFIED_ANALYSIS_DIR, 'y_test_prediction_correct.npy'))
    
    # Check if SMOTE-balanced data exists
    smote_path = os.path.join(MODIFIED_ANALYSIS_DIR, 'X_train_smote.npy')
    if os.path.exists(smote_path):
        X_train_smote = np.load(smote_path)
        y_train_correct_smote = np.load(os.path.join(MODIFIED_ANALYSIS_DIR, 'y_train_prediction_correct_smote.npy'))
        print("Loaded SMOTE-balanced data.")
    else:
        print("SMOTE-balanced data not found. Using original training data.")
        X_train_smote = X_train
        y_train_correct_smote = y_train_correct
    
    # Load feature names
    feature_names_path = os.path.join(MODIFIED_ANALYSIS_DIR, 'transformed_feature_names.csv')
    if os.path.exists(feature_names_path):
        feature_names = pd.read_csv(feature_names_path)['feature'].tolist()
    else:
        print("Feature names file not found. Using generic feature names.")
        feature_names = [f"feature_{i}" for i in range(X_train.shape[1])]
        
    print(f"Loaded data with {X_train.shape[1]} features, {len(y_train_correct)} training samples, and {len(y_test_correct)} test samples.")
    
except Exception as e:
    print(f"Error loading data: {e}")
    print("Please make sure you've run data_cleaning.py first to generate the preprocessed files.")
    exit(1)

# 2. Define and train the classification models
print("\nTraining classification models...")

# Dictionary to store models and their parameters
models_config = {
    'Random Forest': {
        'model': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),
        'params': {'n_estimators': 100, 'max_depth': 10}
    },
    'Logistic Regression (L1)': {
        'model': LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42),
        'params': {'penalty': 'l1', 'C': 0.1}
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),
        'params': {'n_estimators': 100, 'learning_rate': 0.1}
    },
    'PCA + Logistic Regression': {
        'model': Pipeline([
            ('pca', PCA(n_components=0.95, random_state=42)),
            ('lr', LogisticRegression(random_state=42))
        ]),
        'params': {'pca__n_components': 0.95}
    }
}

# Train all models and collect results
results = {}
for name, config in models_config.items():
    print(f"\nTraining {name}...")
    model = config['model']
    
    # Train on SMOTE-balanced data (or original if SMOTE not available)
    model.fit(X_train_smote, y_train_correct_smote)
    
    # Predict on test set
    y_pred = model.predict(X_test)
    
    # Calculate probabilities for ROC and PR curves
    y_proba = model.predict_proba(X_test)[:, 1]
    
    # Compute metrics
    accuracy = accuracy_score(y_test_correct, y_pred)
    conf_matrix = confusion_matrix(y_test_correct, y_pred)
    report = classification_report(y_test_correct, y_pred, output_dict=True)
    roc_auc = roc_auc_score(y_test_correct, y_proba)
    avg_precision = average_precision_score(y_test_correct, y_proba)
    
    # Store results
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'confusion_matrix': conf_matrix,
        'classification_report': report,
        'roc_auc': roc_auc,
        'avg_precision': avg_precision,
        'y_pred': y_pred,
        'y_proba': y_proba
    }
    
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  ROC AUC: {roc_auc:.4f}")
    print(f"  Average Precision: {avg_precision:.4f}")
    print(f"  Confusion Matrix:\n{conf_matrix}")
    
    # Show classification report
    print("  Classification Report:")
    cls_report = pd.DataFrame(report).T
    print(cls_report)

# 3. Compare models
# Create a comparison dataframe
comparison_data = []
for model_name, result in results.items():
    model_data = {
        'Model': model_name,
        'Accuracy': result['accuracy'],
        'ROC AUC': result['roc_auc'],
        'Avg Precision': result['avg_precision']
    }
    
    # Add F1 scores
    report = result['classification_report']
    # Get actual class labels, which could be integers, strings, or something else
    class_labels = [key for key in report.keys() if key not in ['accuracy', 'macro avg', 'weighted avg']]
    
    if len(class_labels) >= 2:
        # Identify which class is for correct predictions (usually 1 or '1')
        correct_class = None
        incorrect_class = None
        
        for label in class_labels:
            # Try to determine if this is the "correct predictions" class
            # For binary classification, higher label typically means positive class
            if str(label) in ['1', 'True', 'Yes', 'Positive']:
                correct_class = label
            elif str(label) in ['0', 'False', 'No', 'Negative']:
                incorrect_class = label
        
        # If we couldn't identify by name, use the order (assuming binary classification)
        if correct_class is None or incorrect_class is None:
            # Sort labels to get consistent ordering
            sorted_labels = sorted(class_labels)
            incorrect_class = sorted_labels[0]
            correct_class = sorted_labels[1]
        
        # Now add the F1 scores
        model_data['F1 (Incorrect)'] = report[incorrect_class]['f1-score']
        model_data['F1 (Correct)'] = report[correct_class]['f1-score']
    
    comparison_data.append(model_data)

comparison_df = pd.DataFrame(comparison_data)
comparison_df = comparison_df.sort_values('ROC AUC', ascending=False)

print("\nModel Comparison:")
print(comparison_df)

# Save comparison to CSV
comparison_path = os.path.join(RESULTS_DIR, 'model_comparison.csv')
comparison_df.to_csv(comparison_path, index=False)
print(f"Model comparison saved to {comparison_path}")

# 4. Plot ROC curves for all models
plt.figure(figsize=(10, 8))
for name in results:
    fpr, tpr, _ = roc_curve(y_test_correct, results[name]['y_proba'])
    plt.plot(fpr, tpr, label=f"{name} (AUC = {results[name]['roc_auc']:.4f})")

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Classification Models')
plt.legend()
plt.grid(True, alpha=0.3)
roc_path = os.path.join(RESULTS_DIR, 'model_comparison_roc_curves.png')
plt.savefig(roc_path, dpi=300)
plt.close()
print(f"ROC curves saved to {roc_path}")

# 5. Feature importance analysis - split into separate functions for better organization
def analyze_rf_importance(model, feature_names):
    """Analyze Random Forest feature importance"""
    if not isinstance(model, RandomForestClassifier):
        print("Model is not a Random Forest. Skipping this analysis.")
        return None
    
    print("\nAnalyzing Random Forest feature importance...")
    rf_importances = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=rf_importances.head(20))
    plt.title('Top 20 Features - Random Forest')
    plt.tight_layout()
    output_path = os.path.join(RESULTS_DIR, 'rf_feature_importance.png')
    plt.savefig(output_path, dpi=300)
    plt.close()
    
    # Save importance scores
    rf_importances.to_csv(os.path.join(RESULTS_DIR, 'rf_feature_importance.csv'), index=False)
    print(f"Random Forest feature importance saved to {RESULTS_DIR}")
    
    return rf_importances

def analyze_logistic_coefficients(model, feature_names):
    """Analyze Logistic Regression coefficients"""
    if not isinstance(model, LogisticRegression):
        if isinstance(model, Pipeline) and isinstance(model.named_steps.get('lr'), LogisticRegression):
            print("Model is a Pipeline with Logistic Regression. Extracting coefficients...")
            model = model.named_steps['lr']
        else:
            print("Model is not a Logistic Regression. Skipping this analysis.")
            return None
    
    print("\nAnalyzing Logistic Regression coefficients...")
    coef = model.coef_[0]
    l1_importance = pd.DataFrame({
        'feature': feature_names,
        'coefficient': coef
    })
    
    # Sort by absolute coefficient value
    l1_importance['abs_coef'] = l1_importance['coefficient'].abs()
    l1_importance = l1_importance.sort_values('abs_coef', ascending=False)
    
    # Plot top coefficients
    plt.figure(figsize=(12, 8))
    top_coefs = l1_importance.head(20)
    sns.barplot(x='coefficient', y='feature', data=top_coefs)
    plt.title('Top 20 Features - Logistic Regression')
    plt.tight_layout()
    output_path = os.path.join(RESULTS_DIR, 'l1_logistic_coefficients.png')
    plt.savefig(output_path, dpi=300)
    plt.close()
    
    # Save coefficients
    l1_importance.to_csv(os.path.join(RESULTS_DIR, 'l1_logistic_coefficients.csv'), index=False)
    print(f"Logistic Regression coefficients saved to {RESULTS_DIR}")
    
    return l1_importance

def analyze_gb_importance(model, feature_names):
    """Analyze Gradient Boosting feature importance"""
    if not isinstance(model, GradientBoostingClassifier):
        print("Model is not a Gradient Boosting. Skipping this analysis.")
        return None
    
    print("\nAnalyzing Gradient Boosting feature importance...")
    gb_importances = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=gb_importances.head(20))
    plt.title('Top 20 Features - Gradient Boosting')
    plt.tight_layout()
    output_path = os.path.join(RESULTS_DIR, 'gb_feature_importance.png')
    plt.savefig(output_path, dpi=300)
    plt.close()
    
    gb_importances.to_csv(os.path.join(RESULTS_DIR, 'gb_feature_importance.csv'), index=False)
    print(f"Gradient Boosting feature importance saved to {RESULTS_DIR}")
    
    return gb_importances

def analyze_pca_components(model, feature_names):
    """Analyze PCA components and their loadings"""
    if not isinstance(model, Pipeline) or not isinstance(model.named_steps.get('pca'), PCA):
        print("Model is not a PCA-based pipeline. Skipping this analysis.")
        return
    
    print("\nAnalyzing PCA components...")
    pca = model.named_steps['pca']
    
    # Get explained variance ratio
    explained_variance = pca.explained_variance_ratio_
    
    plt.figure(figsize=(10, 6))
    plt.bar(range(1, len(explained_variance) + 1), explained_variance)
    plt.xlabel('Principal Component')
    plt.ylabel('Explained Variance Ratio')
    plt.title('PCA Explained Variance')
    output_path = os.path.join(RESULTS_DIR, 'pca_explained_variance.png')
    plt.savefig(output_path, dpi=300)
    plt.close()
    
    # Analyze component loadings for top components
    n_components = min(5, pca.n_components_)  # Show top 5 or fewer
    loadings = pd.DataFrame(
        data=pca.components_[:n_components, :],
        columns=feature_names
    )
    
    # For each component, get top features by absolute loading
    top_loadings = []
    for i in range(n_components):
        component_loadings = pd.DataFrame({
            'feature': feature_names,
            'loading': loadings.iloc[i, :],
            'abs_loading': abs(loadings.iloc[i, :])
        }).sort_values('abs_loading', ascending=False)
        
        top_loadings.append(component_loadings.head(10).copy())
        top_loadings[-1]['component'] = f"PC{i+1}"
    
    # Combine all components' top loadings
    all_top_loadings = pd.concat(top_loadings)
    all_top_loadings.to_csv(os.path.join(RESULTS_DIR, 'pca_top_loadings.csv'), index=False)
    print(f"PCA component analysis saved to {RESULTS_DIR}")

def perform_shap_analysis(model, model_name, X_train, X_test, feature_names):
    """Perform SHAP analysis for the model"""
    print(f"\nPerforming SHAP analysis for {model_name}...")
    
    try:
        # For tree-based models
        if isinstance(model, (RandomForestClassifier, GradientBoostingClassifier)):
            # Use a subset of training data for explainer initialization to improve performance
            train_sample = X_train[np.random.choice(X_train.shape[0], min(1000, X_train.shape[0]), replace=False)]
            
            # Use TreeExplainer for tree-based models
            explainer = shap.TreeExplainer(model)
            
            # Use a subset of test data for SHAP values calculation
            test_sample = X_test[:min(100, X_test.shape[0])]
            shap_values = explainer.shap_values(test_sample)
            
            # If shap_values is a list (for multi-class), get the values for class 1
            if isinstance(shap_values, list):
                shap_values = shap_values[1]
            
            # Create SHAP summary plot
            plt.figure(figsize=(12, 10))
            shap.summary_plot(
                shap_values, 
                test_sample, 
                feature_names=feature_names, 
                max_display=20,
                show=False  # Don't show, just save
            )
            output_path = os.path.join(RESULTS_DIR, f'shap_summary_{model_name.replace(" ", "_").lower()}.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            # Create SHAP dependence plots for top features
            # Get mean absolute SHAP values for feature importance ranking
            mean_abs_shap = np.abs(shap_values).mean(0)
            feature_importance = pd.DataFrame({
                'feature': feature_names,
                'importance': mean_abs_shap
            }).sort_values('importance', ascending=False)
            
            # Save SHAP-based feature importance
            feature_importance.to_csv(
                os.path.join(RESULTS_DIR, f'shap_importance_{model_name.replace(" ", "_").lower()}.csv'), 
                index=False
            )
            
            print(f"SHAP analysis for {model_name} completed and saved to {RESULTS_DIR}")
            return feature_importance
            
        else:
            # For non-tree-based models, use KernelExplainer but with smaller samples
            # Sample both training and test data for performance
            train_sample = X_train[np.random.choice(X_train.shape[0], min(500, X_train.shape[0]), replace=False)]
            test_sample = X_test[np.random.choice(X_test.shape[0], min(50, X_test.shape[0]), replace=False)]
            
            # Create background distribution with kmeans
            background = shap.kmeans(train_sample, k=min(50, len(train_sample)))
            
            # Use KernelExplainer
            explainer = shap.KernelExplainer(
                model.predict_proba, 
                background
            )
            
            # Calculate SHAP values (only for a small number of test samples)
            shap_values = explainer.shap_values(test_sample[:20])
            
            # If SHAP values is a list, get values for class 1 (positive)
            if isinstance(shap_values, list):
                shap_values = shap_values[1]
            
            # Create and save SHAP summary plot
            plt.figure(figsize=(12, 10))
            shap.summary_plot(
                shap_values, 
                test_sample[:20], 
                feature_names=feature_names, 
                max_display=20,
                show=False
            )
            output_path = os.path.join(RESULTS_DIR, f'shap_summary_{model_name.replace(" ", "_").lower()}.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"SHAP analysis for {model_name} completed and saved to {RESULTS_DIR}")
            
            # Return feature importance based on mean absolute SHAP values
            mean_abs_shap = np.abs(shap_values).mean(0)
            feature_importance = pd.DataFrame({
                'feature': feature_names,
                'importance': mean_abs_shap
            }).sort_values('importance', ascending=False)
            
            feature_importance.to_csv(
                os.path.join(RESULTS_DIR, f'shap_importance_{model_name.replace(" ", "_").lower()}.csv'), 
                index=False
            )
            
            return feature_importance
            
    except Exception as e:
        print(f"Error in SHAP analysis for {model_name}: {e}")
        import traceback
        traceback.print_exc()
        return None

# 6. Perform feature importance analysis for each model
print("\nPerforming feature importance analysis...")

# Get the best model based on ROC AUC
best_model_name = comparison_df.iloc[0]['Model']
print(f"Best model based on ROC AUC: {best_model_name}")

# Analyze Random Forest if available
if 'Random Forest' in results:
    rf_importance = analyze_rf_importance(results['Random Forest']['model'], feature_names)

# Analyze Logistic Regression if available
if 'Logistic Regression (L1)' in results:
    lr_importance = analyze_logistic_coefficients(results['Logistic Regression (L1)']['model'], feature_names)

# Analyze Gradient Boosting if available
if 'Gradient Boosting' in results:
    gb_importance = analyze_gb_importance(results['Gradient Boosting']['model'], feature_names)

# Analyze PCA components if available
if 'PCA + Logistic Regression' in results:
    analyze_pca_components(results['PCA + Logistic Regression']['model'], feature_names)

# 7. Perform SHAP analysis for best model only (to save time and avoid errors)
best_model = results[best_model_name]['model']
shap_importance = perform_shap_analysis(best_model, best_model_name, X_train, X_test, feature_names)

# 8. Compare feature importance rankings across methods
importance_methods = {}

# Collect available importance rankings
if 'Random Forest' in results and 'rf_importance' in locals():
    importance_methods['Random Forest'] = rf_importance

if 'Gradient Boosting' in results and 'gb_importance' in locals():
    importance_methods['Gradient Boosting'] = gb_importance

if 'Logistic Regression (L1)' in results and 'lr_importance' in locals():
    # For logistic regression, use absolute coefficient value
    lr_imp = lr_importance.copy()
    lr_imp = lr_imp[['feature', 'abs_coef']].rename(columns={'abs_coef': 'importance'})
    importance_methods['Logistic Regression'] = lr_imp

if 'shap_importance' in locals() and shap_importance is not None:
    importance_methods['SHAP'] = shap_importance

# If we have multiple methods, create a comparison
if len(importance_methods) > 1:
    print("\nComparing feature importance rankings across methods...")
    
    # Get top 15 features from each method
    top_features = {}
    for method, imp_df in importance_methods.items():
        top_features[method] = imp_df.head(15)['feature'].tolist()
    
    # Find features that appear in multiple methods
    all_top_features = []
    for method, features in top_features.items():
        all_top_features.extend(features)
    
    # Count frequency of each feature
    from collections import Counter
    feature_counts = Counter(all_top_features)
    
    # Features that appear in multiple methods
    common_features = [feature for feature, count in feature_counts.items() if count > 1]
    
    print(f"Features important across multiple methods: {len(common_features)}")
    for feature in common_features:
        methods = [method for method, features in top_features.items() if feature in features]
        print(f"  {feature}: {', '.join(methods)}")
    
    # Create a dataframe showing rankings across methods
    rankings = {}
    for method, imp_df in importance_methods.items():
        # Get ranks (1-based)
        imp_df['rank'] = imp_df['importance'].rank(ascending=False)
        # Create a mapping from feature to rank
        rankings[method] = dict(zip(imp_df['feature'], imp_df['rank']))
    
    # Create comparison dataframe for common features
    comparison_rows = []
    for feature in common_features:
        row = {'feature': feature}
        for method in rankings:
            row[f"{method} rank"] = rankings[method].get(feature, np.nan)
            row[f"{method} importance"] = importance_methods[method].set_index('feature').loc[feature, 'importance'] if feature in importance_methods[method]['feature'].values else np.nan
        comparison_rows.append(row)
    
    # Create dataframe and sort by frequency of appearance
    importance_comparison = pd.DataFrame(comparison_rows)
    importance_comparison['methods_count'] = importance_comparison.apply(
        lambda row: sum(1 for col in row.index if 'rank' in col and not pd.isna(row[col])),
        axis=1
    )
    importance_comparison = importance_comparison.sort_values(['methods_count', 'feature'], ascending=[False, True])
    
    # Save comparison
    importance_comparison.to_csv(os.path.join(RESULTS_DIR, 'feature_importance_comparison.csv'), index=False)
    print(f"Feature importance comparison saved to {os.path.join(RESULTS_DIR, 'feature_importance_comparison.csv')}")

# 9. Save all models
for name, model_dict in results.items():
    model = model_dict['model']
    model_path = os.path.join(RESULTS_DIR, f"{name.replace(' ', '_').lower()}_model.joblib")
    joblib.dump(model, model_path)
    print(f"Saved {name} model to {model_path}")

print("\nModel training, evaluation, and feature importance analysis complete!")
print(f"All results saved to {RESULTS_DIR}")