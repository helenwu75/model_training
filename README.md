# Polymarket Election Prediction Analysis

This repository contains a comprehensive analysis of Polymarket election prediction markets, examining factors that contribute to market accuracy and developing a framework for evaluating prediction quality.

## Project Structure

The project is organized into multiple analysis components:
polymarket-analysis/
├── modified_analysis/ # Preprocessed data files
├── feature_importance_results/ # Feature importance analysis
├── feature_relationships/ # Feature correlation and interaction analysis
├── prediction_error_analysis/ # Model error analysis
├── actionable_insights/ # Practical scoring system and thresholds
├── descriptive_stats/ # Descriptive statistics for markets and features
└── scripts/ # Analysis scripts

## Key Findings

- Market accuracy is strongly influenced by price dynamics (price_range, price_fluctuations, price_volatility)
- High trader participation (unique_traders_count) correlates with prediction quality
- Markets with balanced trading (buy_sell_ratio, trader_concentration) tend to have more accurate predictions
- Late-stage market behavior (volume_acceleration, final_week_momentum) significantly impacts prediction accuracy

## Usage

1. Data preprocessing: `python data_cleaning.py`
2. Feature importance analysis: `python feature_analysis.py`
3. Feature relationship analysis: `python feature_relationships.py`
4. Prediction error analysis: `python prediction_error_analysis.py`
5. Actionable insights: `python actionable_insights.py`

## Models

The analysis employs multiple models to ensure robust feature importance assessment:

- Random Forest (best performer: RMSE=0.094, R²=0.357): Robust ensemble method with different feature importance metrics
- Gradient Boosting (RMSE=0.102, R²=0.242): Capturing non-linear relationships
- Lasso Regression (RMSE=0.105, R²=0.201): Creates sparse, interpretable models

These models are used to predict both:

- **Brier score**: A proper scoring rule measuring forecast accuracy
- **Binary prediction correctness**: Whether the market correctly predicted the outcome

# Modified Analysis Directory

This directory contains processed data files ready for model training and analysis. These files are generated by the `data_cleaning.py` script, which performs preprocessing, feature engineering, and train-test splitting on the original election market dataset.

## Directory Contents

project/
├── modified_analysis/ # Preprocessed data ready for modeling
│ ├── X_train_preprocessed.npy # Preprocessed training features
│ ├── X_test_preprocessed.npy # Preprocessed test features
│ ├── y_train_brier_score.npy # Training target (Brier score)
│ ├── y_test_brier_score.npy # Test target (Brier score)
│ ├── transformed_feature_names.csv # Feature names after preprocessing
│ └── X_train_original.csv # Original features for interpretation
│
├── feature_importance_results/ # Results from feature importance analysis
│ ├── model_performance/ # Model evaluation metrics and comparisons
│ ├── feature_importance/ # Model-specific importance measures
│ ├── shap_analysis/ # SHAP visualizations and values
│ ├── consensus_ranking/ # Consensus feature rankings
│ └── saved_models/ # Trained model files
│
└── feature_importance_analysis.ipynb # Analysis notebook

### Core Datasets

| File                         | Description                                             |
| ---------------------------- | ------------------------------------------------------- |
| `complete_election_data.csv` | Complete dataset with all original columns              |
| `cleaned_election_data.csv`  | Cleaned dataset with only modeling features and targets |

### Feature Data

| File                       | Description                                                                       |
| -------------------------- | --------------------------------------------------------------------------------- |
| `X_train_original.csv`     | Original (pre-standardized) training features with index                          |
| `X_test_original.csv`      | Original (pre-standardized) test features with index                              |
| `X_train_preprocessed.npy` | Training features after preprocessing (standardized, one-hot encoded)             |
| `X_test_preprocessed.npy`  | Test features after preprocessing (standardized, one-hot encoded)                 |
| `X_train_smote.npy`        | SMOTE-balanced training features for classification (if class imbalance detected) |

### Target Variables

| File                                   | Description                                                      |
| -------------------------------------- | ---------------------------------------------------------------- |
| `y_train_brier_score.csv/npy`          | Training target - Brier score (regression)                       |
| `y_test_brier_score.csv/npy`           | Test target - Brier score (regression)                           |
| `y_train_log_loss.csv/npy`             | Training target - Log loss (regression)                          |
| `y_test_log_loss.csv/npy`              | Test target - Log loss (regression)                              |
| `y_train_prediction_correct.csv/npy`   | Training target - Binary prediction correctness (classification) |
| `y_test_prediction_correct.csv/npy`    | Test target - Binary prediction correctness (classification)     |
| `y_train_prediction_correct_smote.npy` | SMOTE-balanced training target for classification                |

### Metadata

| File                            | Description                                                        |
| ------------------------------- | ------------------------------------------------------------------ |
| `train_identifiers.csv`         | Identifiers for training samples (market IDs, questions)           |
| `test_identifiers.csv`          | Identifiers for test samples (market IDs, questions)               |
| `transformed_feature_names.csv` | Names of features after preprocessing (including one-hot encoding) |
| `original_feature_names.csv`    | Original feature names before preprocessing                        |
| `feature_preprocessor.joblib`   | Scikit-learn preprocessing pipeline for transforming new data      |

## Using These Files

### For Model Training

Use the `.npy` files for model training as they contain properly preprocessed numerical data:

- For classification models: Use `X_train_preprocessed.npy`/`X_test_preprocessed.npy` with `y_train_prediction_correct.npy`/`y_test_prediction_correct.npy`
- For regression models: Use `X_train_preprocessed.npy`/`X_test_preprocessed.npy` with `y_train_brier_score.npy`/`y_test_brier_score.npy` or `y_train_log_loss.npy`/`y_test_log_loss.npy`
- For classification with balanced classes: Use `X_train_smote.npy` with `y_train_prediction_correct_smote.npy`

### For Feature Analysis

To analyze feature importance:

1. Train models using the preprocessed data
2. Extract feature importance
3. Map back to original feature names using `transformed_feature_names.csv`

### For Data Exploration

Use `cleaned_election_data.csv` for exploratory data analysis and descriptive statistics.

### For Preprocessing New Data

To preprocess new data with the same transformations:

```python
import joblib

# Load the preprocessor
preprocessor = joblib.load('feature_preprocessor.joblib')

# Transform new data
X_new_preprocessed = preprocessor.transform(X_new)
```

## Data Preprocessing Steps

The `data_cleaning.py` script performs these preprocessing steps:

1. **Feature Selection**: Removes features that would cause data leakage
2. **Missing Value Handling**: Imputes missing values in numerical and categorical features
3. **Categorical Processing**:
   - Groups rare election types into broader categories
   - Groups rare countries into regional categories
4. **Feature Scaling**: Standardizes numerical features
5. **One-Hot Encoding**: Creates binary features for categorical variables
6. **Train-Test Split**: Creates a 70/30 train/test split (stratified by prediction_correct)
7. **Class Imbalance**: Applies SMOTE to balance classes if needed

## Target Variables

Three target variables are available for modeling:

1. **Brier Score** - Measures squared error of probability predictions (lower is better)
2. **Log Loss** - Measures negative log-likelihood of outcomes (lower is better)
3. **Prediction Correctness** - Binary indicator (1 = correct prediction, 0 = incorrect)

## Output Data

Output Data Organization
The analysis generates multiple output files organized by analysis type:

1. Model Performance Files:

- model_performance_comparison.csv: Metrics for all models (RMSE, MAE, R²)
- model_performance_comparison.png: Visualization of comparative performance

2. Feature Importance Files:

- rf_feature_importance.csv/png: Random Forest feature importance
- gb_feature_importance.csv/png: Gradient Boosting feature importance
- lasso_coefficients.csv/png: Lasso regression coefficients
- permutation*importance*\*.csv/png: Permutation importance for each model

3. SHAP Analysis Files:

- shap*summary*\*.png: Summary plot of SHAP values
- shap*dependence*\*.png: Dependence plots for top features
- shap*importance*\*.csv: Feature importance based on SHAP values

4. Consensus Ranking Files:

- consensus_feature_ranking_full.csv: Complete ranking of all features
- consensus_feature_ranking_top20.csv: Top 20 features by consensus
- consensus_feature_ranking_top15.png: Visualization of top consensus features
- consolidated_feature_importance_heatmap.png: Heatmap comparing importance across methods

  5.Feature Interpretation Files:

- feature_interpretation.csv: Statistical analysis of top features

6. Trained Models:

- \*\_model.joblib: Serialized model files for future use
